{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil b - Adaline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptives Lineares Neuron (Adaline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weiterentwicklung des Perzeptrons\n",
    "\n",
    "Das Perzeptron liefert keine stabile Lösung, falls das Problem nicht linear trennbar ist. Stabil bedeutet, wenn ein neues Trainingsbeispiel betrachtet wird, nicht sofort alles vergisst, was bis zu diesem Zeitpunkt gelernt wurde. Aus diesem Grund haben Widrow und Hoff ein neuronales Netz vorgeschlagen, das genau diese Eigenschaften besitzt. Sie nannten es *adaptive linear* (ADAptive LInear Neuron) bzw. Adaline. Adaline ist ein weiteres neuronales Netz mit einer einzigen Schicht. Adaline wurde nur wenige Jahre nach dem Perzeptron-Algorithmus veröffentlicht und kann als Verbesserung aufgefasst werden. Es bildet die Grundlage für fortgeschrittene Lernalgorithmen für die Klassifizierung, wie bspw. die logistische Regression, Support Vector Machines, Multi-layer Perceptrons, etc.\n",
    "\n",
    "### Änderungen der Gewichtungen\n",
    "Die Berechnung der Änderungen der Gewichte findet bei beiden Algorithmen unterschiedlich statt: <br>\n",
    "\n",
    "<b>Perzeptron</b>: <br>\n",
    "\n",
    "Beim Perzeptron wird der Ouput, dh die Klassifikation des Inputs, zur Fehlerkorrektur verwendet. Das bedeutet, dass ein Binärwert für die Fehlerkorrektur verwendet wird. <br>\n",
    "\n",
    "$\\Delta w_j = \\eta \\cdot (y^{(i)} - \\hat{y}^{(i)}) \\cdot x_j^{(i)}$ <br>\n",
    "\n",
    "<b>Adaline</b>: <br>\n",
    "\n",
    "\n",
    "Beim Adaline wird der Net Input (dh die gewichtete summe $s$ bzw. $net$) zur Fehlerkorrektur verwendet. Das bedeutet, dass ein kontinuierlicher Wert für die Fehlerkorrektur verwendet wird. Das sorgt dafür, dass die Änderungen an den Gewichten besser in Relation zu den Fehlern stehen. Ein weiterer Unterschied beim Adaline-Algorithmus ist, dass die Berechnung der Gewichtsaktualisierung auf allen Trainingsobjekten erfolgt. Aus diesem Grund wird diese Form des Lernalgorithmus als Stapelverarbeitung bezeichnet.\n",
    "\n",
    "\n",
    "$\\Delta w_j = \\eta \\cdot (y - net) \\cdot x_j$ <br>\n",
    "\n",
    "### Lernen im Adaline-Algorithmus\n",
    "\n",
    "Der wesentliche Unterschied des Adaline-Algorithmus besteht darin, dass die Aktualisierung der Gewichtungen auf einer linearen Aktivierungsfunktion beruht. Allerdings ist $\\phi(s)$ mit $s$ = $\\vec{w}^T \\vec{x} +w_0$ eine reele Zahl und keine ganzzahlige Klassenbezeichnung.  Bei Adaline ist diese Funktion $\\phi(s)$ einfach die identische Abbildung der Nettoeingabefunktion, sodass $\\phi(s) = \\vec{w}^T \\vec{x}+w_0$. Die lineare Aktivierungsfunktion wird dazu genutzt die Gewichtungen zu lernen. Anschließend kann eine Schwellenwertfunktion (besitzt Ähnlichkeit mit der bereits bekannten Sprungfunktion) verwendet werden, um die Klassenbezeichnungen vorherzusagen. <br>\n",
    "\n",
    "Bei fortgeschrittenen Lernalgorithmen, wie bspw. dem Multi-layer Perceptron, kann als Aktivierungsfunktion die Sigmoid-Funktion ( [Siehe Web-Link](https://en.wikipedia.org/wiki/Sigmoid_function) ) eingesetzt werden.\n",
    "\n",
    "Folgende Abbildung zeigt, dass der Adaline-Algorithmus die tatsächlichen Klassenzeichnungen mit den stetigen Ausgaben der linearen Aktivierungsfunktion vergleicht. Um Fehler des Modells zu berechnen und die Gewichtungen zu aktualisieren. <br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./Figures/Adaline.png\" alt=\"drawing\" style=\"width:600px;\"/>\n",
    "\n",
    "\n",
    "Zum Vergleich, das Perzeptron aktualisiert die Gewichtungen nach jedem Objekt inkrementell. Des Weiteren beruht beim Perzeptron die Aktualisierung der Gewichtungen auf einer einfachen Sprungfunktion. Das Perzeptron vergleicht die tatsächlichen Klassenzeichnungen mit den vorhergesagten Klassenzeichnungen.\n",
    "\n",
    "<img src=\"./Figures/Perzeptron.png\" alt=\"drawing\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung\n",
    "\n",
    "Die Implementierungen der Adaline-Regel und der Perzeptron-Regel sind sich sehr ähnlich. Die Implementierung erfolgt innerhalb der Klasse <b>Adaline</b>. Im folgenden werden die einzelnen Methoden und deren Funktionsweise kurz vorgestellt. <br>\n",
    "\n",
    "#### Konstruktor\n",
    "Das Perceptron-Objekt wird mit der Lernrate <b>eta</b> und der Anzahl der Epochen (Durchläufe der Trainingsdaten) <b>epochs</b> initialisiert. Wählen Sie geeignete Werte für die Epoche (Anzahl der Durchläufe) und die Lernrate Eta.\n",
    "\n",
    "#### activation()-Methode:\n",
    "Diese Funktion bewirkt im Code nichts, denn es handelt sich um eine Identitätsfunktion. Sie existiert zur Demonstration, wie Informationen durch ein einschichtiges Neuronales Netz fließen: Merkmale der Eingabedaten, Nettoeingabe, Aktivierungsfunktion und Ausgabe. <br>\n",
    "\n",
    "Weitere Klassifizierer (wie bspw. logistische Regression, Multi-layer Perzeptron, etc.) sind sehr eng mit Adaline verwandt. Denn sie unterscheiden sich in der Aktivierungs- und Straffunktion.\n",
    "\n",
    "#### fit()-Methode:\n",
    "\n",
    "<b>Gewichtungen</b>: <br>\n",
    "Die Gewichtungen werden wie beim Perzeptron initialisiert. <br>\n",
    "Die Aktualisierung der Gewichtung der Bias-Einheit wird anhand der Summe der Errors berechnet.<br>\n",
    "Die Aktualisierung der weiteren Gewichtungen von 1 bis m wird anhand <b>X.T.dot(errors)</b> berechnet. Hierbei handelt es sich um eine *Matrix-Vektor-Multiplikation* von Merkmalsmatrix und Fehlervektor. <br>\n",
    "\n",
    "Die Berechnung der Gewichtsaktualisierung erfolgt auf allen Trainingsobjekten. Zum Vergleich, beim Perzeptron werden die Gewichtungen nach der Berechnung jedes einzelnen Trainingsexemplar aktualisiert. <br>\n",
    "\n",
    "Sammeln Sie in einer Liste <b>cost</b> die in jeder Epoche auftretenden quadrierten Fehler nach folgender Formel: <b>cost_epoch= (error**2).sum() / 2.0</b>. Dadurch kann später analysiert werden, wie gut der Adline-Algorithmus während des Trainings funktioniert hat. Geben Sie diese Liste als Rückgabewert der Methode zurück.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaline(object):\n",
    "    \n",
    "    def __init__(self, eta=None, epochs=None):\n",
    "        # TODO: implement\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def gewichtete_summe(self, x):\n",
    "        # TODO: implement\n",
    "        #print(self.w)\n",
    "        w0 = self.w[0] #bias\n",
    "        w_withoutBias = np.delete(self.w, 0) \n",
    "        #print(w0)\n",
    "        #print(w_withoutBias)\n",
    "        summe = np.dot(x, w_withoutBias)\n",
    "        #print(summe)\n",
    "        summe += w0 # bias addieren \n",
    "        #summe = np.round(summe)\n",
    "        #print(summe)\n",
    "        return summe\n",
    "    \n",
    "    def heaviside(self, summe):\n",
    "        # TODO: implement\n",
    "        schwelle = 0\n",
    "        #print(summe)\n",
    "        print(summe)\n",
    "        if summe >= schwelle:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def activation(self, X):\n",
    "        # TODO: implement\n",
    "        return None \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # TODO: implement\n",
    "        bias = 1\n",
    "        w1 = 0.1 # != 0\n",
    "        w2 = 0.1 # != 0\n",
    "        self.w = np.array([bias, w1, w2])\n",
    "        #fehler_gesamt = 0\n",
    "        \n",
    "        errors_list = []\n",
    "        cost_epoch = 0\n",
    "        \n",
    "        for current_ep in range(0,self.epochs): # fuer jede Epoche\n",
    "            sum_fehler = 0\n",
    "            sum_cost = 0\n",
    "            current_index = 0\n",
    "            errors = []\n",
    "            for row in X:\n",
    "                #row = X[row_index]\n",
    "                #print(X.info())\n",
    "                #print(row)\n",
    "                summe = self.gewichtete_summe(row)\n",
    "                #print(summe)\n",
    "                res_perc = self.heaviside(summe) # Ergebnis des Perceptrons \n",
    "                #print(res_perc)\n",
    "                # Fehler berechnen\n",
    "                fehler = y[current_index] - res_perc\n",
    "                if(True):#fehler != 0):\n",
    "                    # Gewichte aktualisieren\n",
    "                    deltaW = self.eta * fehler                    \n",
    "                    self.w[1] += deltaW * row[0]\n",
    "                    self.w[2] += deltaW * row[1]\n",
    "                    errors.append(fehler)\n",
    "                    fehler_quadrat = fehler **2\n",
    "                    fehler = 0 \n",
    "                    sum_fehler += 1\n",
    "                    #sum_cost += fehler_quadrat\n",
    "                    #x_temp = x.copy()\n",
    "                    print(errors)\n",
    "                    X[current_index] = X[current_index].T.dot(errors)             \n",
    "                current_index += 1\n",
    "                \n",
    "            #X = X.T.dot(errors)\n",
    "            print(X)\n",
    "            cost_epoch= sum_cost / 2.0\n",
    "            #errors_list.append(sum_fehler)\n",
    "            errors_list.append(cost_epoch)\n",
    "                \n",
    "        return cost_epoch\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datensatz, Training und Visualisierung des Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wählen Sie denselben Datensatz wie beim Perzeptron. <br>\n",
    "Führen Sie das Training anhand verschiedener Parameter aus und visualisieren Sie die Ergebnisse. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.65\n",
      "[-1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,) and (1,) not aligned: 2 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-76d5066f5daa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m#print(errors)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#plt.line(np.arange(epochs), errors, c='green', s=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-0bbd0d3ec0d0>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     72\u001b[0m                     \u001b[1;31m#x_temp = x.copy()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                     \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m                 \u001b[0mcurrent_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,) and (1,) not aligned: 2 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"./Data/iris.data\", header=None, sep=\",\")\n",
    "\n",
    "df = df.replace('Iris-setosa', 0)\n",
    "df = df.replace('Iris-versicolor', 1)\n",
    "# Auswahl von setosa und versicolor\n",
    "y = np.array(df[0:100:1][4]) # TODO: implement \n",
    "#print(y)\n",
    "# Auswahl von Kelch- und Bluetenblattlaenge\n",
    "df = df.drop(columns=[1,3,4])\n",
    "X = df[0:100:1]\n",
    "\n",
    "#print(X)\n",
    "\n",
    "# TODO: Implement\n",
    "epochs = 1\n",
    "eta = 1 \n",
    "# bei 0.1 entstehen zwei spitzen\n",
    "# bei 0.01 spätere Konvergenz \n",
    "\n",
    "ad = Adaline(eta, epochs)\n",
    "errors = ad.fit(X.values,y)\n",
    "#print(errors)\n",
    "#plt.line(np.arange(epochs), errors, c='green', s=1)\n",
    "plt.plot(np.arange(epochs), errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

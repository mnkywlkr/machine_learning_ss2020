{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil a - Perzeptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Klassifizierung anhand eines Perzeptrons kann formal als binäre oder dichotome Klassifizierung betrachtet werden. Die Ziel-Klassen werden als 1 (positive Klasse) und 0 (negative Klasse) bezeichnet. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufbau des Perceptrons\n",
    "\n",
    "<img src=\"./Figures/perceptron.png\" alt=\"drawing\" style=\"width:800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Input $\\vec{x}$\n",
    "\n",
    "\n",
    "$\\vec{x}$ wird als Inputvektor bezeichnet und repräsentiert die Eingangsdaten. Die Werte werden übereinander geschrieben: Spaltenschreibweise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Gewicht $\\vec{w}$ \n",
    "\n",
    "$\\vec{w}$ wird als Gewichtungsvektor bezeichnet. Gewichte stellen die Verbindungen zwischen zwei Neuronen her. Bspw. sendet Neuron A ein Signal an Neuron B. Das Gewicht steuert hierbei, wie stark das Signal in Neuron B ankommt. Ein Wert 0..1 verringert das Signal, ein Wert >1 verstärkt das Signal. Das Gewicht regelt hierbei die Signalstärke. \n",
    "\n",
    "Lernen in neuronalen Netzen bedeutet die Anpassung der Gewichte.\n",
    "\n",
    "Zusätzlich zur Spaltenschreibweise, wird der Gewichtsvektor in Zeilenschreibweise dargestellt, d.h. die Werte werden nebeneinander geschrieben. Hierzu wird dem Gewichtsvektor ein hochgestelltes <i>T</i> angehängt, dies bedeutet <i>transponiert</i>. \n",
    "\n",
    "Der Grund hierfür ist, dass damit die Multiplikation zwischen den Inputwerten und den Gewichten einfacher beschrieben werden kann: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Gewichtete Summe $s$\n",
    "\n",
    "Die gewichtete Summe bildet die Linearkombination bestehend aus dem Inputvektor $\\vec{x}$ und dem Gewichtungsvektor $\\vec{w}$: <br>\n",
    "s = ${w}_1 \\cdot {x}_1  + {w}_2 \\cdot {x}_2 + ... {w}_n \\cdot {x}_n $.\n",
    "\n",
    "\n",
    "Die Summe aller $w_i \\cdot x_i$ lässt sich kompakt darstellen als: $s = \\sum^{n}_{i=1}w_i \\cdot x_i$. \n",
    " \n",
    "\n",
    "Die Summe der Produkte der Werte von $\\vec{x}$ und $\\vec{w}$ wird als Skalarprodukt zweier Vektoren abgekürzt. Hierbei werden Spaltenvektoren in Zeilenvektoren transformiert: $s= w^T x$.\n",
    "\n",
    "Rechenbeispiel:\n",
    "\n",
    "$\n",
    "\\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix}  \n",
    "\\cdot\n",
    "\\begin{pmatrix} 4 \\\\ 5 \\\\ 6 \\\\ \n",
    "\\end{pmatrix}  \n",
    "= 1 \\cdot 4 + 2\\cdot 5 + 3 \\cdot 6 = 32\n",
    "$\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ermittlung des Skalarprodukts\n",
    "\n",
    "Die Multiplikation zweier Vektoren kann anhand der Numpy-Funktion <b>dot()</b> umgesetzt werden. Die Multiplikation wird als <i>Skalarprodukt</i>, <i>Dot-Product</i> oder <i>inneres Produkt</i> bezeichnet. Skalarprodukt, weil das Ergebnis der Multiplikation ein Skalar (dh. ein Wert) ist, und kein Vektor. Dem gegenüber führt der Multiplikationsoperator in Python <b>*</b> die Operation Element für Element aus.\n",
    "\n",
    "Bilden Sie anhand der Vektoren $\\vec{v}_1= [1,2,3]$ und $\\vec{v}_2 = [4,5,6]$, abgebildet als Numpy-Arrays, das Skalarprodukt und die elementweise Multiplikation und zeigen Sie die Unterschiede auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot-Product of v1 and v2: 32\n",
      "\n",
      "Element-wise multiplication of v1 and v2 : [ 4 10 18]\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array([1,2,3])\n",
    "v2 = np.array([4,5,6])\n",
    "\n",
    "print(f'Dot-Product of v1 and v2: {v1.dot(v2)}\\n')\n",
    "print(f'Element-wise multiplication of v1 and v2 : {np.multiply(v1,v2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Stufenfunktion $step$(s)\n",
    "\n",
    "Für die Stufenfunktion wird folgende Funktion definiert: <br>\n",
    "$\n",
    "    step(s) = f(x) = \\left\\{\\begin{array}{lr}\n",
    "        0, & \\text{falls } s < \\theta \\\\\n",
    "        1, & \\text{falls } s \\geq \\theta\n",
    "        \\end{array}\\right\\} \n",
    "$\n",
    "\n",
    "Unabhängig vom Eingabewert der Stufenfunktion ist das Ergebnis stets entweder 0 oder 1. Jedoch ist sie vom Schwellenwert Theta $\\theta$ abhängig, wann der Sprung von 0 auf 1 stattfindet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Output $f_{akt}(\\vec{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgangslage folgender Überlegung ist die gewichtete Summe mit der Stufenfunktion (Heaviside-Funktion): <br>\n",
    "${w}_1 \\cdot {x}_1  + {w}_2 \\cdot {x}_2 + ... {w}_n \\cdot {x}_n \\geq \\theta$.\n",
    "\n",
    "Durch Umformung der Ungleichung wird $\\theta$ auf die linke Seite gebracht. Das Ergebnis ist ein erweiterter Gewichtsvektor, der um eine Dimension für den Schwellenwert erweitert wird. Die neue Dimension wird am Index 0 eingefügt, sodass der ursprüngliche Vektor inklusive Indizes erhalten bleibt: <br>\n",
    "${w}_1 \\cdot {x}_1  + {w}_2 \\cdot {x}_2 + ... {w}_n \\cdot {x}_n -\\theta \\geq  0$.\n",
    "\n",
    "Damit der Schwellenwert an der Stelle 0 eingefügt werden kann, wird $-\\theta$ in $w_0$ umbenannt. Der Vektor $\\vec{x}$ wird ebenfalls um einen Wert erweitert. Der Wert für den Input $x_0$ wird mit dem Wert 1 besetzt. Somit existiert eine einheitliche Form um bei der Notation $\\vec{w}^T \\cdot \\vec{x}$ zu bleiben. Es gilt: <br>\n",
    "${w}_1 \\cdot {x}_1  + {w}_2 \\cdot {x}_2 + ... {w}_n \\cdot {x}_n +w_0 \\cdot x_0 \\geq  0$.\n",
    "\n",
    "Diese Form der Nullgewichtung wird in der Literatur als <i>Bias-Neuron</i> bezeichnet. Dadurch ergibt sich folgendes Schaubild:\n",
    "\n",
    "![title](./Figures/perceptron_erweitert.png)\n",
    "\n",
    "Die Implementierung auf Code-Ebene weicht davon jedoch etwas ab. <br>\n",
    "Anstatt den Eingangsvektor $\\vec{x}$ mit dem Wert 1 zu erweitern, wird der Wert $w_0$ auf die gewichtete Summe addiert. Formal wiefolgt beschrieben (siehe Skript Prof. Link): \n",
    "\n",
    "$s = \\vec{w}^T \\cdot \\vec{x} + w_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erste Implementierung\n",
    "\n",
    "Im folgenden werden die bisher gewonnenen Erkenntnisse in Code umgesetzt. Die Datengrundlage liefert folgende Tabelle (OR-Problem).\n",
    "\n",
    "<img src=\"./Figures/or-problem.png\" alt=\"drawing\" style=\"width:600px;\"/>\n",
    "\n",
    "Ingesamt handelt es sich hierbei um vier Inputvektoren. \n",
    "\n",
    "Für die Fehlerberechnung der Einzelfehler wird für jeden Input-Vektor der berechnete Output mit dem gewünschten Output verglichen. Das Perceptron kann aufgrund der Heaviside-Funktion entsprechend nur 0 oder 1 ausgeben. Der gewünschte Output ist per Definition ebenfalls 0 oder 1. Somit kann die Differenz nur -1, 0, 1 betragen. In diesem Fall gilt es die Betragsfunktion für den Einzelfehler zu berechnen. Denn sonst würde ein Fehler von -1 den Gesamtfehler verringern. <br>\n",
    "\n",
    "Die Einzelfehler der Input-Vektren werden summiert und somit der Gesamtfehler bestimmt. Der Gesamtfehler stellt die Ermittlungsgenauigkeit des Perzeptrons dar. <br>\n",
    "\n",
    "Die Gewichtungen sind mit Werten zu belegen. Experimentieren sie mit den Gewichtungen, sehen sie sich die Fehler an und versuchen sie diese anhand der besprochenen Berechnungen nachzuvollziehen. Wählen Sie die Gewichte so, sodass das OR-Problem gelöst werden kann. \n",
    "\n",
    "## Implementierung\n",
    "\n",
    "Die Implementierung erfolgt innerhalb der Klasse <b>SimplePerceptron</b>. Im folgenden werden die einzelnen Methoden und deren Funktionsweise kurz vorgestellt. <br>\n",
    "\n",
    "### Konstruktor\n",
    "Hier ist nichts zu implementieren. <br>\n",
    "\n",
    "### gewichtete_summe()-Methode:\n",
    "In dieser Methode soll die beschriebene gewichtete Summe $\\vec{w}^T \\cdot \\vec{x} + w_0$ berechnet werden.\n",
    "\n",
    "### heaviside()-Methode:\n",
    "Heaviside-Funktion als Stufenfunktion, dh Schwellenwert ist 0.\n",
    "\n",
    "### perceptron_eval()-Methode:\n",
    "\n",
    "<b>Gewichtungen</b>: <br>\n",
    "Die Gewichtungen in <b>self.w</b> werden mit einem Vektor $\\mathbb R^{m+1}$ initialisiert, m gibt die Anzahl der Dimensionen (Merkmale) in der Datensammlung an. Dem ersten Element dieses Vektors (dies entspricht der Bias-Einheit) wird ein Wert zugeordnet. Gehen Sie von zwei Merkmalen aus (wie oben beschrieben).<br>\n",
    "\n",
    "Implementieren Sie den besprochenen Perceptron-Algorithmus mit den folgenden Schritten:\n",
    "* Berechnung gewichtete Summe\n",
    "* Anwendung der Heaviside-Funktion\n",
    "* Ermittlung des Fehlers\n",
    "* Ermittlung des Gesamtfehlers\n",
    "\n",
    "Geben Sie die den Gesamtfehler als Rückgabewert der Methode zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePerceptron(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def heaviside(self, summe):\n",
    "        # TODO: implement\n",
    "        schwelle = 0\n",
    "        if summe >= schwelle:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "        \n",
    "    def gewichtete_summe(self, x):\n",
    "        #return x.dot(self.w)+initial_w\n",
    "        # TODO: implement\n",
    "        w0 = self.w[0] #bias\n",
    "        w_withoutBias = np.delete(self.w, 0) \n",
    "        summe = np.dot(x, w_withoutBias)\n",
    "        summe += w0 # bias addieren        \n",
    "        return summe\n",
    "    \n",
    "    \n",
    "    def perceptron_eval(self, X,y):\n",
    "        # TODO: implement\n",
    "        bias = 1\n",
    "        w1 = 0.5 \n",
    "        w2 = 0.1 \n",
    "        self.w = np.array([bias, w1, w2])\n",
    "        fehler_gesamt = 0\n",
    "        current_index = 0\n",
    "        for row in X :\n",
    "            print(row)\n",
    "            summe = self.gewichtete_summe(row)\n",
    "            print(summe)\n",
    "            res_perc = self.heaviside(summe) # Ergebnis des Perceptrons \n",
    "            print(res_perc)\n",
    "            # Fehler berechnen\n",
    "            fehler = abs(y[current_index] - res_perc)\n",
    "            fehler_gesamt += fehler\n",
    "            current_index += 1\n",
    "            \n",
    "        return fehler_gesamt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmus ausführuen und  Gesamtfehler anzeigen\n",
    "\n",
    "Führen Sie den SimplePerceptron-Algorithmus mit den beschriebenen Daten uns und geben Sie den Gesamtfehler aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "1.0\n",
      "1\n",
      "[0 1]\n",
      "1.1\n",
      "1\n",
      "[1 0]\n",
      "1.5\n",
      "1\n",
      "[1 1]\n",
      "1.6\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 2 dimensionaler Input: x1, x2\n",
    "# 4 Inputvektoren\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])# TODO: implement\n",
    "\n",
    "# Die 4 gewünschten Ergebniswerte\n",
    "y = np.array([0,1,1,1]) # TODO: implement\n",
    "\n",
    "# TODO: implement: USE SimplePerceptron\n",
    "sp = SimplePerceptron()\n",
    "res = sp.perceptron_eval(X, y)\n",
    "print(res)\n",
    "# w1 =1 und w2 = 1 --> 1\n",
    "# w1 =2 und w2 = 1 --> 1\n",
    "# w1 =2 und w2 = 0.5 --> 1\n",
    "# w1 =0.5 und w2 = 0.1 --> 1\n",
    "# mit bias = 1 ist es doch immer >= schwelle (=1) und somit immer als Ergbenis 1 und damit auch immer 1 als Fehler?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perzeptron-Lernalgorithmus\n",
    "\n",
    "<img src=\"./Figures/vorwaerts_und_rueckwaerts.png\" alt=\"drawing\" style=\"width:300px;\"/>\n",
    "\n",
    "### Vorwärtspfad und Rückwärtspfad\n",
    "Im vorherigen Teil wurde die Auswertung durch das Netz im Detail besprochen. <br>\n",
    "Der Vorwärtspfad besteht aus den folgenden Schritten:\n",
    "1. den Input $x_i$ bereitstellen.\n",
    "2. die gewichtete Summe $\\vec{w}^T \\cdot \\vec{x} +w_0$ berechnen.\n",
    "3. die Stufenfunktion anwenden.\n",
    "4. Output auswerten.\n",
    "\n",
    "Der Rückwärtspfad, welcher das Lernen des neuronalen Netzes betrifft, besteht aus den folgenden Schritten:\n",
    "1. Fehler ermitteln.\n",
    "2. Darauf basierend die Gewichte ändern.\n",
    "3. Input bei Berechnung des Fehlers miteinbeziehen.\n",
    "\n",
    "### Lernen in neuronalen Netzen\n",
    "Nachdem die Eingaben $\\vec{x}$ durch die Fragestellung vorgegeben sind, können in neuronalen Netzen nur die Gewichtungen angepasst werden. Dh. es geht um die Anpassung von Gewichten, wenn man von Lernen sprechen. Die Gewichte werden folgendermaßen angepasst.\n",
    "\n",
    "1. Dem Netz wird ein Lernbeispiel präsentiert.\n",
    "2. Berechnungen werden durchgeführt.\n",
    "3. Der Errechnete Wert $\\hat{y}$ wird mit dem gewünschten Wert y vergleichen.\n",
    "4. Basierend auf dem Unterschied des Vergleichs erfolgt die Anpassung der Gewichte.\n",
    "\n",
    "Das Netz sollte die Beispiele immer besser lernen und das gewünschte Ergebnis erzeugen. Jeder Durchgang (Epoche) sollte eine Verringerung des Fehlers bewirken. Das Verhalten nennt sich Konvergenz, die Regel Lernalgorithmus.\n",
    "\n",
    "Der Lernalgorithmus kann verbal folgendermaßen beschrieben werden:\n",
    "1. Initialisierung der Gewichtungen und des Schwellenwertes. Für die Initialisierung gibt es verschiedene Strategien, bspw. Werte im Intervall (-1,1).\n",
    "2. Wenn die errechnete Ausgabe eines Neurons und der gewünschte Wert übereinstimmen (z.b. 1 und 1), werden die Gewichte nicht verändert.\n",
    "3. Stimmen die Werte nicht überein: <br>\n",
    "3.a. Ist die Ausgabe 0 und der Wunschwert 1, so werden alle Gewichte erhöht. Dies geschieht, da der ermittelte Wert zu gering ist und eine Veränderung stattfinden muss. Das berechnete Ergebnis fällt somit höher aus. <br>\n",
    "3.b. Ist die Ausgabe 1 und der Wunschwert 0, so werden alle Gewichte verringert.\n",
    "\n",
    "\n",
    "### Perzeptron Lernalgorithmus\n",
    "\n",
    "Das mit einem Schwellenwert versehene Perzeptron-Modell beruht auf folgender Idee: entweder es feuert oder es feuert nicht. Die Perzeptron-Regel kann durch folgende Schritte zusammengefasst werden:\n",
    "1. Die Gewichtungen werden mit kleinen zufälligen Werten initialisiert.\n",
    "2. Mit jedem Trainingsobjekt $x^{(i)}$ werden folgende Schritte durchgeführt: <br>\n",
    "2.a Berechnung des Ausgabewertes $\\hat{y}$. <br>\n",
    "2.b Aktualisierung der Gewichtungen.\n",
    "\n",
    "Die Ausgabe entspricht die von der zuvor definierten Sprungfunktion vorhergesagte Klassenbezeichnung. Die gleichzeitige Aktualisierung der Gewichtungen $w_j$ im Gewichtungsvektor <b>w</b> wird folgendermaßen formal beschrieben:\n",
    "\n",
    "$s^{(i)} = \\vec{w}^T \\cdot \\vec{x}^{(i)} + w_0$ <br>\n",
    "$\\hat{y}^{(i)} = step(s^{(i)})$ <br>\n",
    "$E= y^{(i)} - \\hat{y}^{(i)}$ <br>\n",
    "$\\Delta w_j = \\eta \\cdot E $ <br>\n",
    "$w_j = w_j + \\Delta w_j \\cdot x_j^{(i)} $<br>\n",
    "\n",
    "\n",
    "$\\vec{w}$ der Gewichtsvektor <br>\n",
    "$\\vec{x}^{(i)}$ der <i>i-</i>te Input-Vektor <br>\n",
    "$w_0$ der Bias-Vektor <br>\n",
    "$s^{(i)}$ = gewichtete Summe <br>\n",
    "$E$ Error, Fehler <br>\n",
    "$y^{(i)}$ die gewünschte/ tatsächliche Klassenbezeichnung des <i>i</i>-ten Trainingsobjekts.<br>\n",
    "$\\hat{y}^{(i)}$ die vorhergesagte Klassenbezeichnung.<br>\n",
    "$\\Delta{w_j}$ bezeichnet die stattfindende Änderung des Gewichts $w_j$ und  wird zur Aktualisierung der Gewichtungen verwendet. <br>\n",
    "$\\eta$ Eta Lernrate (eine Konstante zwischen 0.0 und 1.0) <br>\n",
    "$w_j$ der <i>j</i>-te Gewichtsvektor  <br>\n",
    "$x_j^{(i)}$ der <i>j-</i>te Wert  <br>\n",
    "\n",
    "\n",
    "Es ist wichtig anzumerken, dass alle Gewichtungen im Gewichtungsvektor gleichzeitig aktualisiert werden, sodass $y^{(i)}$ nicht erneut berechnet werden muss, bevor alle Gewichtungen $\\Delta w_j$ aktualisiert wurde. <br>\n",
    "\n",
    "### Beispiele\n",
    "#### Allgemeines Beispiel\n",
    "\n",
    "In den beiden Szenarien, in denen das Perzeptron die Klassenbezeichnung korrekt vorhersagt, bleiben die Gewichtungen unverändert: <br>\n",
    "\n",
    "* $w_j = \\eta(-1--1) \\cdot x_j^{(i)} = 0.$\n",
    "* $w_j = \\eta(1-1) \\cdot x_j^{(i)} = 0.$\n",
    "\n",
    "Im Falle einer falschen Vorhersage werden die Gewichtungen in Richtung der positiven bzw. negativen Zielklasse verschoben: \n",
    "\n",
    "* $w_j = \\eta(1--1) \\cdot x_j^{(i)} = \\eta(2) \\cdot x_j^{(i)}.$\n",
    "* $w_j = \\eta(-1-1) \\cdot x_j^{(i)} = \\eta(-2) \\cdot x_j^{(i)}.$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Konkretes Beispiel 1\n",
    "\n",
    "Ein weiteres Beispiel um ein bessers Gespür für den multiplikativen Faktor $x_j^{(i)}$ zu bekommen: <br>\n",
    "\n",
    "Es gilt: $y^{(i)}$= +1; $\\hat{y}^{(i)}$ = -1; $\\eta$= 1.\n",
    "\n",
    "Angenommen $x_j^{(i)}$= 0.5 und dieses Objekt wird mit -1 klassifiziert. Somit ergibt sich folgende Berechnung zur Aktualisierung des Gewichts: <br>\n",
    "\n",
    "$\\Delta w_j = (1--1) \\cdot 0.5 = (2) \\cdot 0.5 = 1.$\n",
    "\n",
    "In diesem Fall wird die zugehörige Gewichtung um 1 erhöht. Somit wird die Nettoeingabe $x_j^{(i)} \\cdot w_j$ positiver, wenn das nächste Mal auf das Objekt getroffen wird. Und somit die Wahrscheinlichkeit erhöht, dass der Schwellenwert der Sprungfunktion überschritten und das Objekt als +1 klassifiziert wird. Die Aktualisierung der Gewichtungen erfolgt proportional zum Wert $x_j^{(i)}$.\n",
    "\n",
    "#### Konkretes Beispiel 2\n",
    "Weiteres Beispiel: \n",
    "$x_j^{(i)}$=2 wird irrtürmlich als -1 klassifiziert. Update-Berechnung wiefolgt:\n",
    "$\\Delta w_j = (1--1) \\cdot 2 = (2) \\cdot 2 = 4.$ <br>\n",
    "Die Aktualisierung des Objekts erfolgt sogar noch stärker.\n",
    "\n",
    "\n",
    "### Zusammenfassung\n",
    "Folgende Abbildung illustriert die Funktionsweise des Perzeptrons:\n",
    "* nimmt Eingabe $\\vec{x}$ entgegen.\n",
    "* kombiniert diese mit den Gewichtungen $\\vec{w}$.\n",
    "* berechnet die Nettoeingabefunktion.\n",
    "* diese wird an Aktivierungsfunktion (hier: Heaviside-Funktion) übergeben.\n",
    "* erzeugt binäre Ausgabe: 0 oder 1.\n",
    "* dies entspricht der Vorhersage für die Klassenbezeichnung.\n",
    "* Während der Lernphase wird die Ausgabe genutzt, um\n",
    "* a) den Fehler festzustellen\n",
    "* b) die Gewichtungen zu aktualisieren\n",
    "\n",
    "<img src=\"./Figures/Perzeptron.png\" alt=\"drawing\" style=\"width:600px;\"/>\n",
    "\n",
    "\n",
    "Es gilt zu beachten, dass die Konvergenz des Perzeptrons nur dann gewährleistet ist, wenn beide Klassen linear trennbar sind. Sollte dies nicht der Fall sein, kann eine maximale Anzahl an Durchläufen der Trainingsdaten (Epochen) oder ein Schwellenwert für die Anzahl der Fehlklassifizierungen definiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung\n",
    "\n",
    "Die Implementierung erfolgt innerhalb der Klasse <b>Perceptron</b>. Im folgenden werden die einzelnen Methoden und deren Funktionsweise kurz vorgestellt. <br>\n",
    "\n",
    "### Konstruktor\n",
    "Das Perceptron-Objekt wird mit der Lernrate <b>eta</b> und der Anzahl der Epochen (Durchläufe der Trainingsdaten) <b>epochs</b> initialisiert. Wählen Sie geeignete Werte für die Epoche (Anzahl der Durchläufe) und die Lernrate Eta.\n",
    "\n",
    "### gewichtete_summe()-Methode:\n",
    "In dieser Methode soll die beschriebene gewichtete Summe $\\vec{w}^T \\cdot \\vec{x} + w_0$ berechnet werden. Zur Berechnung des Skalarprodukts zweier Arrays ergeben sich zwei Möglichkeiten. <br>\n",
    "\n",
    "* Python: via <b>sum(...)</b>. Berechnung wird mit den einzelnen Elementen durchgeführt.<br>\n",
    "* Numpy: via <b>np.dot(a,b)</b>. Hat den Vorteil, dass arithmetische Operationen vektorisiert sind. Vektorisierung bedeutet, dass arithmetische Operationen automatisch auf alle Elemente eines Arrays angewendet werden. Durch die Formulierung der arithmetischen Operationen als eine Reihe von Array-Rechenanweisungen kann die Fähigkeit moderner CPUs besser genutzt werden. Darüber hinaus verwendet Numpy hochoptimierte Bibliotheken für lineare Algebra wie bspw. BLAS und LAPACK, welche in C oder Fortran implementiert sind.<br>\n",
    "\n",
    "\n",
    "### heaviside()-Methode:\n",
    "Implementierung der Heaviside-Funktion. Parameter ist die gewichtete Summe . Es soll die vorhergesagte Klassenbezeichnungen für den Eingangsvektor $\\vec{x}$  ermittelt werden. \n",
    "\n",
    "### fit()-Methode:\n",
    "\n",
    "<b>Gewichtungen</b>: <br>\n",
    "\n",
    "Die Gewichtungen in <b>w</b> werden mit einem Vektor $\\mathbb R^{m+1}$ initialisiert, m gibt die Anzahl der Dimensionen (Merkmale) in der Datensammlung an. Dem ersten Element dieses Vektors (dies entspricht der Bias-Einheit) wird der Wert 1 zugeordnet. Gehen Sie von zwei Merkmalen aus (weiter unten wird die Datenstruktur beschrieben).<br>\n",
    "\n",
    "Die Gewichtungen werden nicht mit null initialisiert, weil sich die Lernrate Eta $\\eta$ nur dann auf das Ergebnis der Klassifizierung auswirkt, wenn die Gewichtungen von Null verschiedene Werte besitzt. <br>\n",
    "\n",
    "Überlegen Sie sich geeignete Initialisierungswerte für die Gewichtungen. Bspw. können die Werte der Gewichtungen einer Normalverteilung entnommen werden. Als Standardabweichung kann hierbei 0.01 dienen. <br>\n",
    "\n",
    "Hinweis: die Normalverteilung kann via <b>np.random.normal</b> oder <b>np.random.RandomState</b> und <b>randomstate.rgen.normal</b> erzeugt werden.\n",
    "\n",
    "<b>Funktionsweise</b>: <br>\n",
    "\n",
    "In dieser Methode soll das Training des neuronalen Netzes umgesetzt werden. <br>\n",
    "\n",
    "Pro Epoche werden alle Trainingsobjekte durchlaufen und die Gewichtungen gemäß der Perzeptron-Lernregel aktualisiert. Innerhalb der <b>fit()</b>-Methode werden die zuvor definierten Methoden aufgerufen, um die Klassenbezeichnung für die Aktualisierung der Gewichtungen vorherzusagen und die Gewichtungen zu aktualisieren. <br>\n",
    "\n",
    "Sammeln Sie in einer Liste <b>errors</b> die in jeder Epoche auftretenden Fehlklassifizierungen. Dadurch kann später analysiert werden, wie gut das Perzeptron während des Trainings funktioniert hat. Geben Sie diese Liste als Rückgabewert der Methode zurück.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    \n",
    "    #def __init__(self, eta=None, epochs=None):\n",
    "    def __init__(self, eta, epochs):\n",
    "        # TODO : implement\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        pass\n",
    "        \n",
    "    def gewichtete_summe(self, x):\n",
    "        # TODO: implement\n",
    "        #print(self.w)\n",
    "        w0 = self.w[0] #bias\n",
    "        w_withoutBias = np.delete(self.w, 0) \n",
    "        #print(w0)\n",
    "        #print(w_withoutBias)\n",
    "        summe = np.dot(x, w_withoutBias)\n",
    "        #print(summe)\n",
    "        summe += w0 # bias addieren        \n",
    "        return summe\n",
    "        \n",
    "    def heaviside(self, summe):\n",
    "        # TODO: implement\n",
    "        schwelle = 0\n",
    "        #print(summe)\n",
    "        #print(summe)\n",
    "        if summe >= schwelle:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # TODO: implement\n",
    "        bias = 1\n",
    "        w1 = 0.1 # != 0\n",
    "        w2 = 0.1 # != 0\n",
    "        self.w = np.array([bias, w1, w2])\n",
    "        #fehler_gesamt = 0\n",
    "        errors = []\n",
    "        \n",
    "        for current_ep in range(0,self.epochs): # fuer jede Epoche\n",
    "            sum_fehler = 0\n",
    "            current_index = 0\n",
    "            for row in X:\n",
    "                #row = X[row_index]\n",
    "                #print(X.info())\n",
    "                #print(row)\n",
    "                summe = self.gewichtete_summe(row)\n",
    "                #print(summe)\n",
    "                res_perc = self.heaviside(summe) # Ergebnis des Perceptrons \n",
    "                #print(res_perc)\n",
    "                # Fehler berechnen\n",
    "                fehler = y[current_index] - res_perc\n",
    "                if(fehler != 0):\n",
    "                    # Gewichte aktualisieren\n",
    "                    deltaW = self.eta * fehler                    \n",
    "                    self.w[1] += deltaW * row[0]\n",
    "                    self.w[2] += deltaW * row[1]\n",
    "                    #errors.append(fehler)\n",
    "                    sum_fehler += 1\n",
    "                    \n",
    "                current_index += 1\n",
    "            errors.append(sum_fehler)\n",
    "                \n",
    "        return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datensatz\n",
    "\n",
    "Für die folgende Implementierung wird der Iris-Datensatz verwendet. Dieser ist unter /Data abgelegt. Lesen Sie den Datensatz ein.\n",
    "\n",
    "Zur Implementierung werden je zwei Klassen und zwei Merkmale des Iris-Datensatz als zweidimensionalen Merkmalsraum verwendet. Dies geschieht aus praktischen Gründen wie bspw. die Nachvollziehbarkeit der Werte und die Visualisierung der Merkmale.\n",
    "Selektieren Sie aus dem Datensatz die beiden Klassen *Setosa* und *Versicolor* und  hiervon die beiden Merkmale \"Länge des Kelchblatts\" und \"Länge des Blütenblatts\". \n",
    "\n",
    "Wählen Sie für die Bestimmung der Eingabematrix <b>X</b> die Merkmale Kelch- und Bluetenblattlaenge (Sepal Length und Petal Length) aus. Wählen für die Bestimmung des Zielvektors $\\vec{y}$ die Werte von setosa und versicolor. Setzen Sie hierfür eine Zielvariable auf 1, die andere auf 0.\n",
    "\n",
    "Visualisieren Sie den entstandenen Merkmalsraum, so dass die verschiedenen Zielvariablen unterscheidbar sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "      0    2\n",
      "0   5.1  1.4\n",
      "1   4.9  1.4\n",
      "2   4.7  1.3\n",
      "3   4.6  1.5\n",
      "4   5.0  1.4\n",
      "..  ...  ...\n",
      "95  5.7  4.2\n",
      "96  5.7  4.2\n",
      "97  6.2  4.3\n",
      "98  5.1  3.0\n",
      "99  5.7  4.1\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: implement\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"./Data/iris.data\", header=None, sep=\",\")\n",
    "\n",
    "df = df.replace('Iris-setosa', 0)\n",
    "df = df.replace('Iris-versicolor', 1)\n",
    "# Auswahl von setosa und versicolor\n",
    "y = np.array(df[0:100:1][4]) # TODO: implement \n",
    "print(y)\n",
    "# Auswahl von Kelch- und Bluetenblattlaenge\n",
    "df = df.drop(columns=[1,3,4])\n",
    "X = df[0:100:1]\n",
    "\n",
    "print(X)\n",
    "\n",
    "# Diagramm ausgeben\n",
    "# TODO: implement\n",
    "x_setosa = df[0:50:1][0]\n",
    "y_setosa = df[0:50:1][2]\n",
    "x_versicolor = df[50:100:1][0]\n",
    "y_versicolor = df[50:100:1][2]\n",
    "\n",
    "# Plot\n",
    "plt.scatter(x_setosa, y_setosa, c='red', s=1)\n",
    "plt.scatter(x_versicolor, y_versicolor, c='green', s=1)\n",
    "\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Petal Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Beispiel: <br>\n",
    "<img src=\"./Figures/Sepal-Length-Petal-Length.png\" alt=\"drawing\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training und Visualisierung des Errors\n",
    "\n",
    "Führen Sie das Training anhand der Perzeptron-Klasse und der <b>fit</b>-Methode mit unterschiedlichen Parametern für die Epoche (bspw. 10, 30, 100, etc.) und die Lernrate (bspw. 1, 0.01, 0.00000001, etc.) durch. Visualisieren Sie die von der <b>fit</b>-Methode zurückgegebenen Errors in einem Plot (d.h. x-Achse=Epochen; y-Achse=Fehler). Dieser Plot ist von großer Bedeutung. Hierbei können Sie die Leistungsfähigkeit Ihrer Implementierung prüfen.\n",
    "\n",
    "Vergleichen Sie die Ergebnisse mit den unterschiedlich gewählten Parametern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement\n",
    "epochs = 10\n",
    "eta = 1 \n",
    "# bei 0.1 entstehen zwei spitzen\n",
    "# bei 0.01 spätere Konvergenz \n",
    "\n",
    "pcp = Perceptron(eta, epochs)\n",
    "errors = pcp.fit(X.values,y)\n",
    "print(errors)\n",
    "#plt.line(np.arange(epochs), errors, c='green', s=1)\n",
    "plt.plot(np.arange(epochs), errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beispiel Error-Plot: <br>\n",
    "\n",
    "<img src=\"./Figures/example-error-plot.png\" alt=\"drawing\" style=\"width:400px;\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv as csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation mit Cross-Validation\n",
    "Um verschiede Verfahren und Parameter möglichst ohne die Gefahr des overfitting evaluieren zu können, steht man immer vor dem Problem: Mit welchen Daten trainiere ich meine Verfahren und mit welchen teste ich? Offensichtlich hängt das Ergebnis der Evaluation stark von der konkreten Auswahl des Test- bzw. Trainingsdatensatzes ab. \n",
    "\n",
    "Eine in der Literatur etablierte Methode der systematischen Evaluation ist Cross-Validation (Link: [Cross-Validation](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29)). Die grundlegende Idee des k-Fold  Cross-Validation (Link: [k-fold cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)\\#k-fold_cross-validation)) ist wie folgt: Die Gesamtmenge an Klassen-annotierten Datensätzen $T$ wird zufällig in $k$ gleich große Teilmengen (Folds) $T_1 \\dots T_k$ aufgeteilt. Es werden $k$ Testiteration $i_1 \\dots i_k$ durchgeführt. In jeder Iteration wird jeweils eine andere Teilmenge $T_i$ als Testdatensatz und die restlichen Daten $T \\setminus T_i$ als Trainingsdatensatz verwendet. Als Gesamt Ergebniss der Cross-Validation wird der Mittelwert der Genauigkeiten der einzelnen Iteration herangezogen. \n",
    "\n",
    "Weitere Verfahren sind bspw. Holdout (Link: [Holdout](https://en.wikipedia.org/wiki/Cross-validation_(statistics)\\#Holdout_method)), Nested cross-validation (Link: [Nested cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)\\#Nested_cross-validation)) etc.\n",
    "\n",
    "<figure>\n",
    "<img src=\"./Figures/k-fold-cross-validation.png\" alt=\"drawing\" style=\"width:600px;\">\n",
    "    <figcaption>k-fold Cross Validation, Quelle: https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/K-fold_cross_validation_EN.svg/500px-K-fold_cross_validation_EN.svg.png\n",
    "        </figcaption>\n",
    "</figure>\n",
    "\n",
    "Erweitern Sie Ihre Implementierung des KNN-Algorithmus aus dem vorherigen Teil um das <b>k-fold Cross-Validation</b> Verfahren. Wählen Sie hierbei einen geeigneten Wert für die Anzahl der k-folds, bzw. experimentieren Sie mit verschiedenen Werte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = './Data/original_titanic.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "# (1) Datenlücken interpolieren\n",
    "def prepareData(df):\n",
    "    # TODO: implement\n",
    "    df['Age'] = df.groupby(['Sex','Pclass', 'Survived'])['Age'].transform(lambda x:x.fillna(x.median()))\n",
    "    return df\n",
    "\n",
    "df_prepared = prepareData(df)\n",
    "\n",
    "# (2) Datensatz stochastisch verändern\n",
    "df_shuffled =  df.reindex(np.random.permutation(df.index)) # TODO: implement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare one-hot-encodings for Sex, Pclass and Embarked\n",
    "df_with_dummies = pd.get_dummies(df_shuffled, columns = [\"Sex\",\"Pclass\",\"Embarked\"], \\\n",
    "                                 prefix=[\"Sex_type\",\"Pclass_type\",\"Em_type\"])\n",
    "\n",
    "df_shuffled = df_with_dummies.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_train=df_shuffled.sample(frac=0.8,random_state=1)\n",
    "df_test=df_shuffled.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature : which features to keep\n",
    "def extractFeatureVector(row):\n",
    "    return [row['Age'], row['Fare'], row['Sex_type_female'], row['Sex_type_male'],\\\n",
    "            row['Pclass_type_1'], row['Pclass_type_2'], row['Pclass_type_3'],\\\n",
    "            row['Em_type_C'], row['Em_type_Q'], row['Em_type_S']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN(object):\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def distance(self, vector1, vector2):\n",
    "        return np.sqrt(np.nansum((vector1 - vector2)**2))\n",
    "    \n",
    "    \n",
    "    def fit(self, df):\n",
    "        trainSet = []\n",
    "        for i in range (len(df)):\n",
    "            trainSet.append(extractFeatureVector(df.iloc[i]))\n",
    "    \n",
    "        self.trainData = np.array(trainSet)\n",
    "        self.trainLabel = np.array(df['Survived'])\n",
    "\n",
    "    \n",
    "    def _euclidean_dist(x, y):\n",
    "        return \n",
    "\n",
    "    def predict(self, x):\n",
    "        dist_to_train_sample = [(self.distance(x, train_sample), i) for i, train_sample in enumerate(self.trainData)]\n",
    "        sorted_dist = sorted(dist_to_train_sample, key=lambda x: x[0])\n",
    "        \n",
    "        k_nearest = sorted_dist[:self.k]\n",
    "#         print(k_nearest)\n",
    "        \n",
    "        k_nearest_labels = [self.trainLabel[i] for _, i in k_nearest]\n",
    "#         print(k_nearest_labels)\n",
    "        \n",
    "        ones = (np.sum(np.array(k_nearest_labels)))\n",
    "        zeros = self.k - ones\n",
    "\n",
    "        #tie breaker, return random\n",
    "        if ones == zeros:\n",
    "            return np.random(k_nearest_labels)\n",
    "        \n",
    "        return 1 if (ones > zeros) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_normalize(df, df_train):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['Age'] = (df['Age'] - np.mean(df_train['Age']))/np.std(df_train['Age'])\n",
    "    df_copy['Fare'] = (df['Fare'] - np.mean(df_train['Fare']))/np.std(df_train['Fare'])\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN(k=5)\n",
    "\n",
    "iteration = 11\n",
    "len_test = int(len(df_shuffled)/iteration)\n",
    "classification_rates = []             \n",
    "\n",
    "for i in range(iteration):\n",
    "    df_test = df_shuffled[len_test*i :len_test*i + len_test]\n",
    "    if i != 0:\n",
    "        part1 = df_shuffled[:len_test*i]\n",
    "        part2 = df_shuffled[len_test*i + len_test:]\n",
    "        df_train = pd.concat([part1, part2])\n",
    "    else:\n",
    "        df_train = df_shuffled[len_test*i + len_test:]\n",
    "        \n",
    "    df_train_normed = calculate_normalize(df_train, df_train)    \n",
    "    df_test_normed = calculate_normalize(df_test, df_train)\n",
    "#     print(f'iteration {i+1}/{iteration}')\n",
    "    \n",
    "    knn.fit(df_train_normed)\n",
    "    correct_prediction = 0\n",
    "    for i in range(len(df_test)):\n",
    "        survival_prediction = knn.predict(extractFeatureVector(df_test_normed.iloc[i]))\n",
    "        if survival_prediction == df_test_normed.iloc[i]['Survived']:\n",
    "            correct_prediction += 1\n",
    "\n",
    "    classification_rates.append(correct_prediction / len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Boat', 'Body',\n",
       "       'Home-Dest'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_normed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7815126050420168, 0.8235294117647058, 0.865546218487395, 0.7983193277310925, 0.7899159663865546, 0.8235294117647058, 0.7899159663865546, 0.8235294117647058, 0.865546218487395, 0.7563025210084033, 0.8235294117647058]\n"
     ]
    }
   ],
   "source": [
    "print(classification_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8128342245989305"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(classification_rates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
